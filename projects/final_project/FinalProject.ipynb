{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ca6dae",
   "metadata": {},
   "source": [
    "# Final Project: Great British Bake Off üë©‚Äçüç≥üç∞üá¨üáß\n",
    "\n",
    "## Due: Tuesday, March 14 at 11:59PM\n",
    "\n",
    "___\n",
    "\n",
    "## Outline of the Project \n",
    "\n",
    "This project has eight sections. Use the outline below to help you quickly navigate to the part of the project you're working on. Most questions are worth one point. Those shown in bold in the outline are worth two points. Most of these are harder, more complex questions, but sometimes they are simple questions that test correctness of previously implemented functions.\n",
    "\n",
    "-  [Welcome üëã](#welcome)\n",
    "-  [About the Show üì∫](#about_show)\n",
    "-  [About the Data üíæ](#about_data)\n",
    "-  Section 1. [Exploratory Data Analysis üîé](#section1)  \n",
    "     - Q1.1, Q1.2, Q1.3, Q1.4, Q1.5\n",
    "-  Section 2. [Popular Ingredients üçä üç´](#section2) \n",
    "     - Q2.1, Q2.2, Q2.3, **Q2.4**, Q2.5, **Q2.6**, Q2.7, Q2.8, Q2.9 \n",
    "-  Section 3. [Gender Balance üë©‚öñÔ∏èüßëüèº](#section3)  \n",
    "     - **Q3.1**, Q3.2, Q3.3, **Q3.4**, Q3.5, Q3.6\n",
    "-  Section 4. [Well-Deserved? ü•á](#section4)  \n",
    "     - **Q4.1**, Q4.2, **Q4.3**, Q4.4, **Q4.5**, **Q4.6**\n",
    "-  Section 5. [Devilishly Difficult Challenges üòà](#section5)  \n",
    "     - Q5.1, **Q5.2**, Q5.3, Q5.4, Q5.5, Q5.6, Q5.7, **Q5.8**, Q5.9, Q5.10, Q5.11, **Q5.12**\n",
    "-  Section 6. [Piece of Cake? üç∞](#section6) \n",
    "     - Q6.1, Q6.2, **Q6.3**, Q6.4, **Q6.5**, Q6.6, Q6.7\n",
    "-  Section 7. [Recipe Name Generator üë©‚Äçüç≥üñ®Ô∏è](#section7) \n",
    "     - **Q7.1**, Q7.2, Q7.3, Q7.4, Q7.5\n",
    "-  Section 8. [Dishwashing üßºüçΩÔ∏è](#section8) \n",
    "     - Q8.1, Q8.2, Q8.3, Q8.4, Q8.5, **Q8.6**, Q8.7, **Q8.8**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b81fa6",
   "metadata": {},
   "source": [
    "<a id='welcome'></a>\n",
    "## Welcome üëã\n",
    "\n",
    "Welcome to the Final Project! Projects in DSC 10 are similar in format to homeworks, but are different in a few key ways. First, a project is comprehensive, meaning that it draws upon everything we've learned this quarter so far. Second, since problems can vary quite a bit in difficulty, some problems will be worth more points than others. Finally, in a project, the problems are more open-ended; they will usually ask for some result, but won't tell you what method should be used to get it. There might be several equally-valid approaches, and several steps might be necessary. This is closer to how data science is done in \"real life\".\n",
    "\n",
    "It is important that you **start early** on the project! It is the final assignment that is due this quarter, but it is due just a few days before the Final Exam. You are especially encouraged to **find a partner** to work through the project with. You can work with anyone from any section of the course, but you must follow the [Project Partner Guidelines](https://dsc10.com/project-partners/) on the course website. In particular, you are both required to actively contribute to all parts of the project, and you are not allowed to split up the problems and each work on certain problems. If working in a pair, you should submit one notebook to Gradescope for the both of you. \n",
    "\n",
    "**Important:** The `otter` tests don't usually tell you that your answer is correct. More often, they help catch basic mistakes. It's up to you to ensure that your answer is correct. If you're not sure, ask someone (not for the answer, but for some guidance about your approach). Directly sharing answers between groups is not okay, but discussing problems with the course staff or with other students is encouraged.\n",
    "\n",
    "Please do not import any additional packages - you don't need them, and our autograder may not be able to run your code if you do.\n",
    "\n",
    "As you work through this project, there are a few resources you may want to have open:\n",
    "- [`babypandas` Notes](https://notes.dsc10.com/front.html)\n",
    "- [Course Textbook](https://inferentialthinking.com/chapters/intro.html)\n",
    "- [DSC 10 Reference Sheet](https://drive.google.com/file/d/1mQApk9Ovdi-QVqMgnNcq5dZcWucUKoG-/view)\n",
    "- [`babypandas` Documentation](https://babypandas.readthedocs.io/en/latest/)\n",
    "- Other links in the [Resources](https://dsc10.com/resources/) and [Debugging](https://dsc10.com/debugging/) tabs of the course website\n",
    "\n",
    "Start early, good luck, and let's get started! üòé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c949a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change this cell; just run it.\n",
    "import babypandas as bpd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8b6d54",
   "metadata": {},
   "source": [
    "<a id='about_show'></a>\n",
    "## About the Show üì∫\n",
    "\n",
    "The Great British Bake Off (known in the US as the Great British Baking Show) is a competition-style television show where amateur bakers participate in themed baking challenges. Each week's episode revolves around a theme; past themes include Bread Week, Cake Week, Vegan Week, and Italian Week. In each episode, the bakers are given three timed challenges based on the week's theme: the Signature Challenge, the Technical Challenge, and the Showstopper Challenge. \n",
    "\n",
    "In the Signature Challenge, the judges broadly specify what the bakers should make, and the bakers have freedom to use flavors, techniques, and recipes as they wish. The Signature Challenge earns its name because it's an opportunity for bakers to express themselves and their unique baking style to both the judges and the viewers at home. Many of the Signature Challenge bakes come from tried-and-tested recipes that contestants like to bake for their friends and families. For example, during Festivals Week in Season 10, the bakers were tasked with creating 24 buns themed around a world festival or holiday. Contestant Henry Bird made [these Chocolate Kardemummabullar](https://thegreatbritishbakeoff.co.uk/recipes/all/henry-chocolate-kardemummabullar/).\n",
    "\n",
    "<img src=\"images/signature_bake.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "In the Technical Challenge, bakers have no idea what they will be asked to create until the timer for the challenge starts. This means they can't prepare for it, and they have to rely on their baking knowledge and intuition. The Technical Challenge earns its name because it tests the bakers' technical knowledge of baking as a discipline.  Each Technical Challenge is posed by one particular judge, and uses a recipe from the judge's own personal collection. Bakers are provided with ingredients and a recipe, which is usually extremely basic, sometimes lacking ingredient measurements or containing single steps like \"make a shortcrust pastry.\" The finished products are judged blind and ranked from worst to best. An example of a Technical Challenge includes [judge Paul Hollywood's Baklava](https://thegreatbritishbakeoff.co.uk/recipes/all/paul-hollywood-baklava/). \n",
    "\n",
    "<img src=\"images/baklava.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    "The third challenge, the Showstopper, is similar to the Signature Challenge, in that bakers are given requirements ahead of time and have freedom to create their own recipes and prepare ahead of time. The main difference is that the Showstoppers are more challenging and larger-scale. The judges are looking for bakes that are breathtaking in both their appearance and their taste. For example, during Bread Week in Season 6, the bakers were asked to create a 3-D bread sculpture. Contestant Paul Jagger impressed the judges and millions of viewers with his *King of the Jungle* lion sculpture.\n",
    "\n",
    "<img src=\"images/lion_bread_sculpture.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "Each episode of the show features all three challenges. The contestants' bakes are tasted and assessed by two judges, and at the end of each episode, the hosts announce who will be eliminated from the competition and who will be recognized with a special award of \"Star Baker\" ‚≠ê (introduced in Season 2). Typically, one contestant is eliminated and one is crowned Star Baker ‚≠ê, but on occasion there have been special cases in which zero or multiple people were eliminated or awarded Star Baker ‚≠ê. \n",
    "\n",
    "The final episode of each season is held when there are just three bakers remaining. All three bakers compete in the final, and at the end, one winner is chosen and each of the others is considered a \"runner-up\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb97f72",
   "metadata": {},
   "source": [
    "<a id='about_data'></a>\n",
    "## About the Data üíæ\n",
    "\n",
    "For this project, we'll be using a few different datasets, which we've loaded in and saved in DataFrames called\n",
    "- `baker_weeks`, \n",
    "- `challenge_results`,  \n",
    "- `technical_challenge_recipes`, and\n",
    "- `bakers`.\n",
    "\n",
    "Note that while the Great British Bake Off has filmed thirteen seasons, our datasets do not include the most recent seasons. Since our datasets come from different [sources](#sources), some of these datasets include more seasons than others. In addition, the number of bakers each season has varied, but all seasons have filmed one episode per week.\n",
    " \n",
    "The `baker_weeks` DataFrame includes a breakdown of each baker's performance each week (that is, each episode), for the first eleven seasons of the show. Each row represents information **for one baker for one week**. This means that each baker will appear in the DataFrame multiple times. Bakers will continue to appear in the DataFrame even in weeks after they got eliminated, so these rows will have missing values (`NaN`).  \n",
    "  \n",
    "The `'Week Name'` column contains the theme of that week's episode. We also have the baker's name, gender (\"M\" or \"F\" are the only options), and age, the season number (also called the series number in other DataFrames), and the week number within that season. There are columns that indicate whether each baker was a Star Baker ‚≠ê that week, was eliminated that week, competed that week, or went on to win the season's competition. A few columns require more explanation about the show:\n",
    "- `'Judge'` is either \"Mary\" or \"Prue\". For the first seven seasons, the show's two judges were [Paul Hollywood](https://en.wikipedia.org/wiki/Paul_Hollywood) and [Mary Berry](https://en.wikipedia.org/wiki/Mary_Berry). After that, the show switched networks and Mary Berry was replaced by [Prue Leith](https://en.wikipedia.org/wiki/Prue_Leith). Since Paul Hollywood was a judge every season, the `'Judge'` column contains the name of the other judge.\n",
    "- `'technical_rank'` contains a number reflecting each baker's ranking in the Technical Challenge (with 1 meaning 1st place, 2 meaning 2nd place, etc.)\n",
    "-  `'Signature Handshake'` and `'Showstopper Handshake'` contain information on whether the contestant received a handshake \tü§ù from judge Paul Hollywood as he tasted their bake. Paul has a reputation for giving praise sparingly, and his so-called \"[Hollywood Handshakes \tü§ù](https://hollywoodhandshakes.com/)\" are considered a great honor. \n",
    "  \n",
    "Run the cell below to load in the `baker_weeks` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197c075",
   "metadata": {},
   "outputs": [],
   "source": [
    "baker_weeks = bpd.read_csv('data/baker_weeks.csv')\n",
    "baker_weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef997795",
   "metadata": {},
   "source": [
    "The `challenge_results` DataFrame contains information on each challenge, with each row representing one baker in one specific episode. As in `baker_weeks`, bakers will reappear multiple times, even after they get eliminated, hence the abundance of `NaN` values. This dataset contains information for the first ten seasons of the show.\n",
    "\n",
    "The `'result'` column indicates whether a baker was eliminated or stayed in the competition. Values of \"OUT\" and \"Runner-up\" mean the baker was eliminated, and values of \"IN\", \"STAR BAKER\", and \"WINNER\" mean that the baker stayed in the competition. There is one instance of \"WD\" in this column for someone who withdrew from the competition, and one instance of \"A\" for someone who was absent one week. We'll ignore both of these.\n",
    "\n",
    "The `'technical'` column contains the baker's rank in the Technical Challenge, and the `'signature'` and `'showstopper'` columns contain the names of the recipes the baker prepared for these challenges.  \n",
    "\n",
    "Run the cell below to load in the `challenge_results` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733f258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge_results = bpd.read_csv('data/challenge_results.csv')\n",
    "challenge_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826830b1",
   "metadata": {},
   "source": [
    "The `technical_challenge_recipes` DataFrame contains information about each recipe that was given as a Technical Challenge in the first nine seasons. The columns specify the season (`'Ssn'`) and episode (`'Ep'`) that each recipe was baked in, which judge's recipe collection it came from (`'Whose'`), and several aspects of the recipe's complexity:\n",
    "- number of components (`'Components'`), which are recipes used within the main recipe, such as a frosting or filling,\n",
    "- number of ingredients (`'IngredCount'`),\n",
    "- number of sentences in the instructions (`'RecipeSentences'`),\n",
    "- number of dirty dishes produced (`'Dishes'`), and\n",
    "- difficulty (`'DifficultyScore'`). \n",
    "\n",
    "Run the cell below to load in the `technical_challenge_recipes` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "technical_challenge_recipes = bpd.read_csv('data/technical_challenge_recipes.csv')\n",
    "technical_challenge_recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3eacb4",
   "metadata": {},
   "source": [
    "The `bakers` DataFrame contains a row for each baker from the first ten seasons, with detailed information about their results in the show, particularly about their performance in the Technical Challenge:\n",
    "- `'technical_winner'`: number of times they won,\n",
    "- `'technical_top3'`: number of times they placed in the top three,\n",
    "- `'technical_bottom'`: number of times they placed last,\n",
    "- `'technical_highest'`: highest (best) rank they ever earned,\n",
    "- `'technical_lowest'`: lowest (worst) rank they ever earned, and\n",
    "- `'technical_median'`: median of all ranks they ever earned.\n",
    "\n",
    "It also includes information about when they appeared on the show and their demographics such as `'occupation'` and `'hometown'`.\n",
    "\n",
    "Run the cell below to load in the `bakers` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033d7d06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bakers = bpd.read_csv('data/bakers.csv')\n",
    "bakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a955fee",
   "metadata": {},
   "source": [
    "Our data comes from a variety of different [sources](#sources) and may contain errors. If you find any errors in the data, do not attempt to fix them; just analyze the data you are given. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d7305",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## Section 1: Exploratory Data Analysis üîé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e87223",
   "metadata": {},
   "source": [
    "To start, we‚Äôll perform some exploratory data analysis to get better acquainted with our data.\n",
    "\n",
    "A common sentiment among long-time viewers of the show is that the baking challenges are getting harder over time. Does the data support this? \n",
    "\n",
    "**Question 1.1.** Using the `technical_challenge_recipes` DataFrame, create an overlaid line plot that shows the season number on the horizontal axis and on the vertical axis:\n",
    "- average number of dirty dishes produced by recipes in that season, \n",
    "- average number of components in recipes in that season,\n",
    "- average number of ingredients in recipes in that season, and \n",
    "- average difficulty score of recipes in that season."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82f5c8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_1\n",
    "points: 1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your overlaid line plot here.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5de2d7",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "Some of the recipes the contestants bake are quite complicated. Let's look at some especially long recipe titles.\n",
    "\n",
    "**Question 1.2.** Using the `challenge_results` DataFrame, which Signature Challenge recipe had the longest name? Save the result as `longest_signature`. \n",
    "\n",
    "Similarly, which Showstopper Challenge recipe had the longest name? Save the result as `longest_showstopper`. In both cases, longest means having the most individual characters, including punctuation and whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_signature = ...\n",
    "print(\"Longest signature name: \", longest_signature, \"\\n\")\n",
    "\n",
    "longest_showstopper = ...\n",
    "print(\"Longest showstopper name: \", longest_showstopper, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9761a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e65d1d",
   "metadata": {},
   "source": [
    "Notice that each of these recipes actually involves multiple items. Often the bakers have to make displays of baked goods with multiple components as part of a single challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2269e4",
   "metadata": {},
   "source": [
    "Another common sentiment among viewers is that the show favors younger people üëßüèΩ. To further explore the bakers' ages, let's convert the `'age'` column to a categorical variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1001680",
   "metadata": {},
   "source": [
    "**Question 1.3.** Add an additional column called `'age_category'` to the `bakers` DataFrame, based on the following age categorization:\n",
    "\n",
    "| Age            | Category    |\n",
    "| -------------- | ----------- |\n",
    "| (0, 39]        | Young       |\n",
    "| (39, 59]       | Middle-Aged |\n",
    "| (59, $\\infty$] | Elderly     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc9e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bakers = ...\n",
    "bakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacbdc9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692a8261",
   "metadata": {},
   "source": [
    "**Question 1.4.** Using the information in the new `'age_category'` column, set `age_prop` to a Series indexed by `'age_category'`, where the values are the proportions of bakers in each `'age_category'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b8a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_prop = ...\n",
    "age_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182a2db",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376cfa78",
   "metadata": {},
   "source": [
    "You should see that a majority of the participants are young!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46acc995",
   "metadata": {},
   "source": [
    "Next, we'll investigate baker occupations. Do bakers on the show tend to hold certain types of jobs? Maybe they work in the food industry, do a lot of cooking at home, or have creative jobs like an artist üé® or photographer üì∑. Some baking challenges even require significant feats of construction üèóÔ∏è, so maybe architects or engineers are popular.\n",
    "\n",
    "**Question 1.5.** Using the `bakers` DataFrame, create an array of occupations held by more than one contestant on the show. Save the array in a variable called `popular_jobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23fcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_jobs = ...\n",
    "popular_jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262699f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2dd9f",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## Section 2: Popular Ingredients üçä üç´\n",
    "\n",
    "\n",
    "Now, we'll try to answer some questions about popular ingredients used in bakers' recipes, and whether there's any connection between certain ingredients and a baker's success in the competition. Our data doesn't exactly include ingredient lists, but we do have recipe titles for the Signature and Showstopper Challenges in `challenge_results`, so we can look for common words there. We'll focus specifically on the Signature Challenge, as it's one in which bakers are able to be creative and showcase a recipe unique to them, and so they have complete freedom to use whatever ingredients they want. \n",
    "\n",
    "\n",
    "The DataFrame below contains all the rows of `challenge_results` with an entry in the `'signature'` column. We've also dropped the columns relating to the Technical and Showstopper Challenges, since we'll be focusing on the Signature Challenge here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = bpd.read_csv('data/signatures.csv')\n",
    "signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5440bf",
   "metadata": {},
   "source": [
    "**Question 2.1.** We want to clean up the text so we can find words that appear frequently in many recipe titles. Write a function named `clean_up_text` that takes the name of a single recipe as input and returns a cleaned-up version of the name with these changes:\n",
    "- Remove any of these characters: `(`, `)`, `'`, `\"`, `;`, `,` (open and close parentheses, single and double quotes, semicolons, commas) \n",
    "- Convert to lowercase.\n",
    "\n",
    "*Hint*: Use the `.replace()` string method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461fe68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_text(recipe):\n",
    "    '''Returns a lowercase version of recipe with certain special characters removed.'''\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594304c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7c3bf",
   "metadata": {},
   "source": [
    "**Question 2.2.** Now that we've created a function to clean the titles, replace the entries in the `'signature'` column of the `signatures` DataFrame with the cleaned version of those recipe titles. Then, assign a new column to the `signatures` DataFrame called `'words'` that contains a list of all the words in the cleaned recipe title, in lowercase. We'll define a word as any chunk of text separated from others by spaces. For example, \n",
    "- a recipe title of `\"Mint, Lilac, & Blackberry Cake\"`\n",
    "- should become `\"mint lilac & blackberry cake\"` when cleaned,\n",
    "- with a corresponding word list of `[\"mint\", \"lilac\", \"&\", \"blackberry\", \"cake\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = ...\n",
    "signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd9578",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c137919",
   "metadata": {},
   "source": [
    "\n",
    "For the next question, you'll need to know something interesting about how lists work in Python: when you sum two lists together, the output is one giant list that contains all the elements in both lists combined. An example is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca702864",
   "metadata": {},
   "outputs": [],
   "source": [
    "['List', 'combining'] + ['is', 'my', \"passion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83572ef2",
   "metadata": {},
   "source": [
    "**Question 2.3.** Combine all the words in the `'words'` column into one big list. Save that list in the variable `all_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4b69b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_words = ...\n",
    "# Just display the first ten words.\n",
    "all_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24c011",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698c745",
   "metadata": {},
   "source": [
    "**Question 2.4.** Write a function called `most_common` that takes as input any list of words, and finds the ten most common words in that list. Your function should output a DataFrame with 10 rows, indexed by `'word'`, with one column called `'count'` containing a count of how many times each word appeared in the input list. Order the rows in descending order of `'count'`.\n",
    "\n",
    "Then use your function to find the ten most common words in `all_words`. These are the words that appeared the most in Signature Challenge recipe titles. Save the resulting DataFrame as `common_words_df`.\n",
    "\n",
    "*Hint*: Leverage the power of `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(word_list):\n",
    "    '''Returns a DataFrame with the ten most common words in word_list, in descending order.'''\n",
    "    ...\n",
    "\n",
    "common_words_df = ...\n",
    "common_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319dbfe3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c262997c",
   "metadata": {},
   "source": [
    "You should find that the most common word is one that doesn't give us any information about the recipe. To deal with that, let's omit common words, which are transition words like \"and\" and \"with\", as well as words like \"cake\" and \"bread\" that appear in the titles of many recipes that were featured in Cake Week or Bread Week.\n",
    "\n",
    "**Question 2.5.** Make a list called `words_to_omit` with all the words that appear anywhere in the `'Week Name'` column of the `baker_weeks` DataFrame. \n",
    "\n",
    "The words in `words_to_omit` should be in all lowercase, regardless of their case in the `'Week Name'` column. Also, `words_to_omit` should not have any duplicate words. Even if a word appears in the `'Week Name'` column multiple times, it should only appear once in `words_to_omit`.\n",
    "\n",
    "For example, one week's theme was \"Pie and Tart\", so the words \"pie\", \"and\", and \"tart\" should all be elements of `words_to_omit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d232d06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words_to_omit = ...\n",
    "# Just display the first ten words.\n",
    "words_to_omit[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a64f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d70b2e",
   "metadata": {},
   "source": [
    "For the next question, you'll need to use the `in` operator in python. The `in` operator checks if a value is an element of a list. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3af09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"macaroni\" in [\"macaroni\", \"and\", \"cheese\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"mac\" in [\"macaroni\", \"and\", \"cheese\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe06c94b",
   "metadata": {},
   "source": [
    "**Question 2.6.** Create a new DataFrame called `meaningful`, with the same data as the `signatures` DataFrame plus an extra column called `'meaningful_words'`, containing a list of all the words that appear in the `'words'` column, except with these words omitted:\n",
    "- \"and\"\n",
    "- \"&\"\n",
    "- \"with\"\n",
    "- any word in `words_to_omit`\n",
    "    \n",
    "*Hint*: Create a function that takes as input one entry of the `'words'` column (a single list of words, corresponding to one recipe title) and returns a list of those same words, except with certain ones omitted. To do that, loop through the words in the list and append the words that should not be omitted to an empty array. Finally, convert the array of non-omitted words to a list before returning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1adab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meaningful = ...\n",
    "meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2032f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9efaf89",
   "metadata": {},
   "source": [
    "**Question 2.7.** Now, find the ten most common words **among only the meaningful ones**. Create a DataFrame called `popular_words` formatted in the same way as `common_words_df`, which you created in Question 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad518f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_words = ...\n",
    "popular_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0833604",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db43dfa9",
   "metadata": {},
   "source": [
    "The most common word should now be the name of a popular ingredient or flavor in British baking. Yum!\n",
    "\n",
    "**Question 2.8.** Now let's try to figure out which meaningful words were most popular in Signature Challenge recipe titles among bakers who were eliminated. These might be harder ingredients or flavors to get right, or ones that are less popular with the judges, and so we might caution future contestants about using these. ‚ö†Ô∏è\n",
    "\n",
    "Use your `most_common` function to produce a DataFrame with the ten most common meaningful words, among Signature Challenge recipes in which the baker was eliminated that week. Name that DataFrame `common_out`.\n",
    "\n",
    "*Hint*: Bakers who are eliminated have a `'result'` of \"OUT\" or \"Runner-up.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_out = ...\n",
    "common_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364560b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be3d4c",
   "metadata": {},
   "source": [
    "**Question 2.9.** Now let's look at the meaningful words that were most popular in Signature Challenge recipe titles among bakers who didn't get eliminated. What special ingredients are they using? These might be more well-loved flavors and ingredients, and we might consider them safe choices for baking foods that the judges will enjoy! üòã\n",
    "\n",
    "Use your `most_common` function to produce a DataFrame with the ten most common meaningful words, among Signature Challenge recipes in which the baker stayed in the competition that week. Name that DataFrame `common_in`.\n",
    "\n",
    "*Hint*: Bakers who stay in the competition have a `'result'` of \"IN\" or \"STAR BAKER\" or \"WINNER\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b51657",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_in = ...\n",
    "common_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d1cbd4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2_9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dcc654",
   "metadata": {},
   "source": [
    "You'll notice that some ingredients are common among people who get eliminated and people who stayed, and that's just because they're common recipe ingredients generally. It's more interesting to look at the words that appear in only one of `common_out` and `common_in`. Would you rather have a walnut mushroom pie or a raspberry rhubarb pudding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132e6eee",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## Section 3: Gender Balance üë©‚öñÔ∏èüßëüèº\n",
    "After watching a couple of episodes, you start to wonder if more female bakers than male bakers have been selected to participate in the Great British Bake Off. Let's check if this is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413cebde",
   "metadata": {},
   "source": [
    "**Question 3.1.** Using the `baker_weeks` DataFrame, first count the total number of bakers in the first 11 seasons of the show and assign your answer to the variable `baker_count`.\n",
    "\n",
    "Then, compute the proportion of female bakers and the proportion of male bakers in the first 11 seasons of the show. Assign your answers to the variables `observed_female_prop` and  `observed_male_prop`. \n",
    "\n",
    "Notice that `baker_weeks` has a row for each baker for each week, so we can't directly calculate proportions from the `'Gender'` column of that DataFrame.\n",
    "\n",
    "*Note*: While several bakers with the same name appeared on the show (there were three Peters and three Kates!) there were never two bakers with the same name appearing in the same season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b85f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_female_prop = ...\n",
    "observed_male_prop = ...\n",
    "baker_count = ...\n",
    "\n",
    "print(\"Female Proportions: \" + str(observed_female_prop))\n",
    "print(\"Male Proprotions: \" + str(observed_male_prop))\n",
    "print(\"Number of Bakers: \" + str(baker_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d71b5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4112f7d",
   "metadata": {},
   "source": [
    "You recognize that `observed_female_prop` and `observed_male_prop` are similar but they're not exactly the same. Is this just random chance at play, or are female bakers actually more likely to be on the show? Let's do a hypothesis test with the following hypotheses:\n",
    "\n",
    "- **Null Hypothesis**: Bakers on the show are drawn randomly from a population that‚Äôs 50% female and 50% male. \n",
    "- **Alternative Hypothesis**: Bakers on the show are not drawn randomly from a population that‚Äôs 50% female and 50% male.\n",
    "\n",
    "Run the cell below to define a variable `null_distribution` that shows the proportion of each gender according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_distribution = np.array([0.5, 0.5])\n",
    "null_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4242fe21",
   "metadata": {},
   "source": [
    "**Question 3.2.** To perform our hypothesis test, we will simulate drawing a random sample of size `baker_count` from the null distribution, and then compute a test statistic on each simulated sample. We must first choose a reasonable test statistic that will help us determine whether to reject the null hypothesis.\n",
    "\n",
    "From the options below, find **all** valid test statistics that we could use for this hypothesis test. Save the numbers of your choices in a `list` called `gender_test_statistics`. Valid test statistics are ones that would allow us to distinguish between the null and alternative hypotheses. \n",
    "\n",
    "*Hint*: To determine whether a test statistic is valid, think about which values of the statistic (high, low, moderate) would make you lean towards the null and which would make you lean towards the alternative.\n",
    "\n",
    "1. The absolute difference between the proportion of female bakers and 0.5.\n",
    "2. The absolute difference between the number of male bakers and the number of female bakers. \n",
    "3. The absolute difference between the number of female bakers and one half of `baker_count`.\n",
    "4. Three times the absolute difference between the proportion of male bakers and 0.5.\n",
    "5. The total variation distance between the gender distribution of bakers and the null distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_test_statistics = ...\n",
    "gender_test_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a2c38",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f969dd",
   "metadata": {},
   "source": [
    "**Question 3.3.** For this hypothesis test, we'll use as our test statistic the absolute difference between the observed proportion of female bakers and 0.5, the expected proportion under the assumptions of the null hypothesis. Set the variable `observed_gender_stat` to the observed value of this statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_gender_stat = ...\n",
    "observed_gender_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd39cdb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72771f27",
   "metadata": {},
   "source": [
    "**Question 3.4.** Write a simulation that runs 10,000 times, each time drawing a random sample of size `baker_count` from the null distribution. Keep track of the simulated test statistics in the `gender_stats` array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_stats = ...\n",
    "    \n",
    "# Visualize with a histogram\n",
    "bpd.DataFrame().assign(gender_stats=gender_stats).plot(kind='hist', density=True, ec='w', figsize=(10, 5));\n",
    "plt.axvline(x=observed_gender_stat, color='black', linewidth=4, label='observed_gender_stat')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a154d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4409798",
   "metadata": {},
   "source": [
    "**Question 3.5.** Recall that your null hypothesis was that the bakers on the show are drawn randomly from a population that‚Äôs 50% female and 50% male. Compute the p-value for this hypothesis test, and save the result to `gender_p_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_p_value = ...\n",
    "gender_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fe54b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e398690",
   "metadata": {},
   "source": [
    "You should find that the p-value is nowhere near the standard cutoff of 0.05 for statistical significance. So in this case, we fail to reject the null. \n",
    "\n",
    "It's important to note that even though we fail to reject the null, we‚Äôre not saying that bakers *were* necessarily drawn randomly from a population that‚Äôs 50% female and 50% male. In fact, nothing is random about how people get to be on the show. \n",
    "\n",
    "There are a lot of rules about who can apply to be on the show, and applicants are thoroughly vetted through an extensive [application process](https://gbbo.take-part.co.uk/info/rules) that involves an interview and a background check to ensure that none of the bakers have any sort of professional training or are friends or relatives of the judges. Simply put, bakers on the show are not selected via a purely random process.\n",
    "\n",
    "When we say we fail to reject the null, this means that the bakers *could have* been drawn from a model that's 50% female and 50% male, but it doesn't mean they *were*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f84c1",
   "metadata": {},
   "source": [
    "**Question 3.6.** Conceptually, how would you expect the statistics in `gender_stats` to change if `baker_count` were a much larger value, like if the show included hundreds of bakers every season? What effect would that have on the result of the hypothesis test?\n",
    "\n",
    "From the options below, save the number of your choice in the variable`gender_stats_change`.\n",
    "\n",
    "1. The values in `gender_stats` would be **smaller**. We'd be **less** likely to reject the null hypothesis if `observed_gender_stat` remained the same.\n",
    "2. The values in `gender_stats` would be **smaller**. We'd be **more** likely to reject the null hypothesis if `observed_gender_stat` remained the same.\n",
    "3. The values in `gender_stats` would be **about the same**. We'd be **equally** likely to reject the null hypothesis if `observed_gender_stat` remained the same.\n",
    "4. The values in `gender_stats` would be **larger**. We'd be **less** likely to reject the null hypothesis if `observed_gender_stat` remained the same.\n",
    "5. The values in `gender_stats` would be **larger**. We'd be **more** likely to reject the null hypothesis if `observed_gender_stat` remained the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ec2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_stats_change = ...\n",
    "gender_stats_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213eece5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2674d8",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## Section 4. Well-Deserved? ü•á"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508f38d",
   "metadata": {},
   "source": [
    "In this section, we will use permutation testing to decide if different groups of bakers have similar technical abilities, as measured by their rankings in the Technical Challenges. Let's start by looking at our `baker_weeks` DataFrame which has a row for each baker for each week of the show, including for the remainder of the season after they've been eliminated. Let's start by only keeping the data for the bakers that actually competed in each week's episode. Since ten bakers participated in the first episode of Season 1, we'll look at the first ten rows of the resulting `competed` DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c5a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "competed = baker_weeks[baker_weeks.get('Competed') == 1]\n",
    "competed.take(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f299bc8d",
   "metadata": {},
   "source": [
    "In the `'Technical Rank'` column, contestants are given a ranking for how well they performed in the Technical Challenge, with 1 being the best. Notice in the first ten rows of `competed` shown above, some of the middle rankings are missing. In this episode, the judges didn't reveal everyone's rank and instead just pointed out the top three and bottom three contestants. For reasons like this, our dataset has just a few missing values, which we will ignore for this section. \n",
    "\n",
    "If we want to get a sense of how skilled a baker is, the technical rank is helpful, but needs to be taken in the context of the number of contestants still in the competition. For example, ranking 3rd place in the first week is a lot more impressive than ranking 3rd place in the final week, when there are just three bakers remaining. To address this problem, we'll convert these rankings into *percentiles* to measure skill relative to the number of contestants remaining. \n",
    "\n",
    "For example, if there are four contestants remaining, a technical ranking of:\n",
    "- 4 corresponds to the 25th percentile\n",
    "- 3 corresponds to the 50th percentile\n",
    "- 2 corresponds to the 75th percentile\n",
    "- 1 corresponds to the 100th percentile\n",
    "\n",
    "**Question 4.1.** Create a DataFrame called `perc` with the same data as `competed`, plus a new column called `'Contestants'`  that contains the number of contestants that competed each week. For example, since the first ten rows of `competed` all correspond to the first week of the first season, in which there were 10 bakers, the first ten entries of the `'Contestants'` column should be 10.  \n",
    "\n",
    "We've provided the code to use the `'Contestants'` column and the `'Technical Rank'` column to calculate the percentiles, which we've added in a column called `'Percentile'`.\n",
    "\n",
    "*Hint*: Start by counting the number of bakers in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565bc82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your task is to add the Contestants column.\n",
    "perc = ...\n",
    "# We've added the Percentile column for you.\n",
    "perc = perc.assign(Percentile = np.round((1 - (perc.get('Technical Rank') - 1) / perc.get('Contestants')) * 100, 1))\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08ff1e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213e2f5",
   "metadata": {},
   "source": [
    "Now we are ready to compare two groups of bakers to see if they are comparably skilled. Let's start with comparing the winners to the non-winners. We'll conduct a permutation test with the following hypotheses.\n",
    "\n",
    "- **Null Hypothesis** : The `'Percentile'` data for winners comes from the same distribution as the `'Percentile'` data for non-winners. In other words, winners and non-winners perform equally well in Technical Challenges.\n",
    "- **Alternate Hypothesis** : The `'Percentile'` data for winners and the `'Percentile'` data for non-winners come from different distributions. Winners perform better in Technical Challenges than non-winners.\n",
    "\n",
    "As usual, we'll use the difference in group means as our test statistic. Here, we'll compute that as the mean for the winners minus the mean for the non-winners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f3683",
   "metadata": {},
   "source": [
    "**Question 4.2.** What is the observed value of the test statistic? Save your answer as `observed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f975817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = ...\n",
    "observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81881b05",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d9e92",
   "metadata": {},
   "source": [
    "**Question 4.3.** Create 1000 simulated values of the test statistic under the assumptions of the null hypothesis, and save your simulated test statistics in the array `simulated_stats`.  Then create an appropriate visualization showing the distribution of the values in `simulated_stats` array. It may be helpful to also plot the observed value of the test statistic on the same graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your simulation here.\n",
    "simulated_stats = ...\n",
    "\n",
    "# Plot your visualization here.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651f5b5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab28374d",
   "metadata": {},
   "source": [
    "**Question 4.4.** The winning contestants claim that they are more technically skilled than the other contestants. Based on your permutation test, using a p-value cutoff of 0.01, do you think this claim is likely accurate? Set `winners_claim` to True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de20a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "winners_claim = ...\n",
    "winners_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d0a2f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd56bc3b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now, we'll do a similar permutation test, but this time comparing the Technical Challenge `'Percentile'` of contestants who received a coveted handshake ü§ù from Paul Hollywood at least once to those who never did.\n",
    "\n",
    "\n",
    "![](https://media.giphy.com/media/3o7TKRoSl2BrFuK75u/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371cc51",
   "metadata": {},
   "source": [
    "**Question 4.5.** Create a new DataFrame called `earned`, indexed by `'Season'` and `'Baker'`, that has a row for each baker who received a handshake ü§ù **at any point** in the season, and a single column called `'Handshake'` containing all ones. \n",
    "\n",
    "Similarly, create a DataFrame called `not_earned`, indexed by `'Season'` and `'Baker'`, that has a row for each baker who **never** received a handshake ü§ù, and a single column called `'Handshake'` containing all zeros. \n",
    "\n",
    "*Note*: There are several bakers by the same name, but never in the same season.\n",
    "\n",
    "*Hint*: Check out the functions [`np.ones`](https://numpy.org/doc/stable/reference/generated/numpy.ones.html) and [`np.zeros`](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed05ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "earned = ...\n",
    "earned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38849b77",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_5_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccffdb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_earned = ...\n",
    "not_earned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38007ff9",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_5_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0710cfa0",
   "metadata": {},
   "source": [
    "Our `earned` and `not_earned` DataFrames contain the information we need to determine who falls into which group for our permutation test, but we need to combine this data with the Technical Challenge percentiles in `perc`. \n",
    "\n",
    "The first step is to combine the rows of `earned` and with those of `not_earned`. We'll do this using the `babypandas` DataFrame method `.append`, which is similar to the familiar `np.append`, but for DataFrames instead of arrays. The cell below puts the rows of `not_earned` onto the end of `earned` and saves the result as `shakes`. Don't worry if you see a warning; ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ae859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shakes = earned.append(not_earned)\n",
    "shakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6369b6e",
   "metadata": {},
   "source": [
    "Now we need to merge `shakes` with `perc` to get the handshake ü§ù data and the percentile data in one DataFrame. Since there are multiple bakers that share a name, we need to merge by *both* `'Season'` and `'Baker'`, which we can do by merging on a `list` containing both column names. Run the cell below to complete the merge and save the result as `perc_shakes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_shakes = perc.merge(shakes, left_on=['Season', 'Baker'], right_index=True)\n",
    "perc_shakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f89049",
   "metadata": {},
   "source": [
    "**Question 4.6.** Now perform a permutation test, mimicking the procedure of Question 4.3, to help you analyze the following claim. \n",
    "\n",
    "The contestants who have gotten a handshake ü§ù claim that they are more technically skilled than the other contestants. Based on your permutation test, using a p-value of cutoff of 0.01, do you think this claim is likely accurate? Set `handshake_claim` to True or False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c300967",
   "metadata": {},
   "outputs": [],
   "source": [
    "handshake_claim = ...\n",
    "handshake_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8db26",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef3d2df",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## Section 5: Devilishly Difficult Challenges üòà\n",
    "\n",
    "Contestants on the Great British Bake Off sometimes groan when the hosts announce that the upcoming Technical Challenge was chosen by judge Paul Hollywood. Paul has a reputation for posing exceptionally difficult challenges and most bakers believe that his recipes are much harder than those of his co-judges, Mary Berry and Prue Leith. We want to examine whether this theory is justified by the data. \n",
    "\n",
    "The `technical_challenge_recipes` DataFrame contains 83 Technical Challenge recipes from seasons 1 through 9. Each Technical Challenge is posed by one particular judge, and comes from their personal collection of recipes. In the first nine seasons, Mary posed 32 Technical Challenges, Paul posed 41, and Prue posed 10. The `technical_challenge_recipes` DataFrame includes a `'DifficultyScore'` for each recipe, with more challenging recipes having higher scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419cb696",
   "metadata": {},
   "source": [
    "**Question 5.1.** Create a DataFrame `mean_by_judge` with the judge's name as the index and just one column called `'mean_difficulty_score'` that contains the mean difficulty score for each judge's Technical Challenges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_by_judge = ...\n",
    "mean_by_judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78fc8b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfbfc5",
   "metadata": {},
   "source": [
    "If you solved this problem correctly, you will notice that Mary and Paul both have an average difficulty of less than 5, whereas Prue has a mean difficulty greater than 7. Does it mean that Prue, in fact, is the devil when it comes to Technical Challenges? In other words, does Prue have a much more challenging recipe collection than the other judges? Or is this all by chance?\n",
    "\n",
    "Suppose each judge has an extensive personal recipe collection with recipes of varying difficulty, and the Technical Challenges for each episode are drawn randomly from this collection. We want to estimate the average difficulty of all recipes in each judge's collection. Unfortunately, we don't have access to a judge's entire recipe collection, we only have access to the sample of recipes they've used for Technical Challenges in the Great British Bake Off. Thus, we will tackle this problem using **bootstrapping**. \n",
    "\n",
    "**Question 5.2.** Below, write a function called `simulate_estimates`. It should take 3 arguments:\n",
    "- `sample_df`: A DataFrame with a row for each element of the original sample. In this case, it will consist of Technical Challenges posed by a particular judge.\n",
    "- `variable`: The column name of the relevant variable, whose mean we want to estimate.\n",
    "- `repetitions`: The number of repetitions to perform (i.e., the number of resamples to create).\n",
    "\n",
    "It should take `repetitions` resamples with replacement from the given DataFrame. For each of those resamples, it should compute the mean of the relevant variable for that resample. Then it should return an array containing the value of those means for each resample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b627d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_estimates(sample_df, variable, repetitions):\n",
    "    '''Returns an array of length repetitions, containing bootstrapped means of the variable from sample_df. '''\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b44314",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4fd91",
   "metadata": {},
   "source": [
    "**Question 5.3.** Use your function `simulate_estimates` to estimate the mean difficulty score of three judges' recipe collections. Use `repetitions = 5000`, and save your arrays of bootstrapped means for each judge in the variables `mary_boot_means`, `paul_boot_means`, and `prue_boot_means`.  \n",
    "\n",
    "Then, plot the distributions of all three of these arrays in one overlaid histogram. Use `bins=np.arange(2,10,0.2)` and set `alpha=0.5` (this changes the opacity to see the distribution more clearly).\n",
    "\n",
    "*Hint:* Create a DataFrame with one column for each judge's bootstrapped means, and use this to plot the histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcbac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mary_boot_means = ...\n",
    "paul_boot_means = ...\n",
    "prue_boot_means = ...\n",
    "\n",
    "# Plot your overlaid histogram here.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649ab8a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e4f8d",
   "metadata": {},
   "source": [
    "**Question 5.4.** Now we want to calculate three 95% confidence intervals for the mean difficulty score of recipes from each of the three judges. To do this, create a function `confidence_interval_95`, which takes in an array of bootstrapped statistics `boot_stats` and returns a list of length two, containing the left endpoint and the right endpoint of the 95% confidence interval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db10f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval_95(boot_stats):\n",
    "    '''Returns a list of the endpoints of a 95% confidence interval based on boot_stats.'''\n",
    "    ...\n",
    "\n",
    "print(\"Mary 95% CI:\", confidence_interval_95(mary_boot_means))\n",
    "print(\"Paul 95% CI:\", confidence_interval_95(paul_boot_means))\n",
    "print(\"Prue 95% CI:\", confidence_interval_95(prue_boot_means))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a260ea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade612df",
   "metadata": {},
   "source": [
    "**Question 5.5.** Based on your results, which of the following statements are correct? Assign `true_statements` to a list containing **all** the true statements. \n",
    "\n",
    "1. Paul's recipes are generally harder than those of his co-judges.\n",
    "2. Prue's recipes are generally harder than those of her co-judges.\n",
    "3. Prue and Mary's confidence intervals overlap.\n",
    "4. Mary and Paul's confidence intervals overlap.\n",
    "5. Mary's confidence interval is wider than Paul's.\n",
    "6. Prue's confidence interval is wider than Mary's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2477052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_statements = ...\n",
    "true_statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb61f8b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a2deb9",
   "metadata": {},
   "source": [
    "**Question 5.6.** If your calculation is correct, you will see that Prue's confidence interval is almost twice as wide as the other two judges' confidence intervals. Why is Prue's confidence interval wider? \n",
    "\n",
    "Assign either 1, 2, or 3 to the variable `why_wider` below.\n",
    "\n",
    "1. She has more challenging recipes in her collection.\n",
    "2. She has posed fewer Technical Challenges.\n",
    "3. She has posed Technical Challenges with a wider range of difficulty levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d690548",
   "metadata": {},
   "outputs": [],
   "source": [
    "why_wider = ...\n",
    "why_wider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d807c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40aceb",
   "metadata": {},
   "source": [
    "From what we've done so far, it's clear that Prue's recipes have a very different difficulty level than the recipes of the other two judges. Now let's address a different question: how does the average difficulty of Paul's recipes compare to the average difficulty of Mary's recipes? \n",
    "\n",
    "**Question 5.7.** Create a DataFrame called `mary_only` containing only the recipes in our original `technical_challenge_recipes` sample from Mary's collection. Then, create another DataFrame called `paul_only` containing only the recipes in our original sample from Paul's collection. Then, set `observed_diff_mean` to the difference in mean difficulty score between Mary's recipes and Paul's recipes in our sample (subtract in the order Mary minus Paul)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e24a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mary_only = ...\n",
    "paul_only = ...\n",
    "observed_diff_mean = ...\n",
    "observed_diff_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7a884",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed69ba",
   "metadata": {},
   "source": [
    "So there is definitely a difference in mean difficulty scores between Mary's and Paul's Technical Challenge recipes, within our sample of recipes that have appeared as Technical Challenges in the show. But does this reflect a difference in mean recipe difficulty scores in the population (the judges' recipe collections), or was it by chance that our sample's difficulty displayed this difference? Let's do a hypothesis test to find out. We'll state our hypotheses as follows:\n",
    "\n",
    "- **Null Hypothesis:** The mean difficulty of Mary's recipe collection equals the mean difficulty of Paul's recipe collection. Equivalently, the difference in the mean difficulty for Mary's and Paul's recipes equals 0.\n",
    "- **Alternative Hypothesis:** The mean difficulty of Mary's recipe collection does not equal the mean difficulty of Paul's recipe collection. Equivalently, the difference in the mean difficulty for Mary's and Paul's recipe does not equal 0.\n",
    "\n",
    "Since we were able to set up our hypothesis test as a question of whether our population parameter ‚Äì the difference in mean difficulty scores for Mary's and Paul's recipe collections ‚Äì is equal to a certain value, we can **test our hypotheses by constructing a confidence interval for the parameter**. This is the method we used in Lecture 19 to test whether the median salary of Fire-Rescue Department workers was the same as the median salary of all San Diego city employees. We also did a similar example in Homework 5 Question 3 (Live Crystal Scoops üîÆ). For a refresher on this method, you can read more about conducting a hypothesis test with a confidence interval in [CIT 13.4](https://inferentialthinking.com/chapters/13/4/Using_Confidence_Intervals.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179eb6d9",
   "metadata": {},
   "source": [
    "**Question 5.8.** Compute 1000 bootstrapped estimates for the difference in the mean difficulty for Mary's recipes and Paul's recipes (subtract in the order Mary minus Paul). Store your 1000 estimates in the `difference_means` array.\n",
    "\n",
    "You should generate your resamples of Mary's recipes by sampling from `mary_only`, and similarly for Paul, by sampling from `paul_only`. You should not use `technical_challenge_recipes` at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(57) # Don't change this. This is for the autograder.\n",
    "\n",
    "difference_means = ...\n",
    "\n",
    "# Just display the first ten differences.\n",
    "difference_means[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c914fc1f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd4d1ad",
   "metadata": {},
   "source": [
    "Let's visualize your estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715584ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "(bpd.DataFrame().assign(DifferenceMeans = difference_means)\n",
    " .plot(kind='hist', density=True, ec='w', figsize=(10, 5)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b41a8",
   "metadata": {},
   "source": [
    "**Question 5.9.** Use the function `confidence_interval_95` you created before to compute a 95% confidence interval for the difference in the mean difficulty of Mary's and Paul's recipes (as before, Mary's minus Paul's). Assign to `mary_paul_difference_CI` a list containing the endpoints of this confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mary_paul_difference_CI = ...\n",
    "mary_paul_difference_CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c2cf4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e6a4d",
   "metadata": {},
   "source": [
    "Recall the hypotheses we were testing:\n",
    "- **Null Hypothesis:** The mean difficulty of Mary's recipe collection equals the mean difficulty of Paul's recipe collection. Equivalently, the difference in the mean difficulty for Mary's and Paul's recipes equals 0.\n",
    "- **Alternative Hypothesis:** The mean difficulty of Mary's recipe collection does not equal the mean difficulty of Paul's recipe collection. Equivalently, the difference in the mean difficulty for Mary's and Paul's recipe does not equal 0.\n",
    "\n",
    "**Question 5.10.** Based on the confidence interval you've created, would you reject the null hypothesis at the 0.05 significance level? Set `reject_null_mary_paul` to True if you would reject the null hypothesis, and False if you would not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56273c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reject_null_mary_paul = ...\n",
    "reject_null_mary_paul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de4ed6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5_10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29a69a",
   "metadata": {},
   "source": [
    "We have now uncovered some interesting facts about the difficulty levels of the different judges' recipe collections. However, we also want to know whether the judges' recipe collections have other differences. For example, do certain judges have recipes with more ingredients, more components, or longer instructions?\n",
    "\n",
    "To do this, we want to generalize our simulation code so that we can create a confidence interval for any variable.\n",
    "\n",
    "**Question 5.11.** Create a function called `bootstrap_estimation`, which takes in 4 inputs:\n",
    "- `sample_df`, A DataFrame with a row for each element of the original sample (Technical Challenge recipes posed by one or more judges)\n",
    "- `judges`, a list of judge's names we want to compare, which can include any number of \"Paul\", \"Mary\", and \"Prue\"\n",
    "- `variable`, the column name of the relevant variable, whose mean we want to estimate \n",
    "- `repetitions`, the number of repetitions to perform (i.e., the number of resamples to create)\n",
    "\n",
    "The function should adhere to these specifications:\n",
    "1. The function should generate an overlaid histogram showing each of the specified judges' simulated means of the given variable. Make sure to give your histogram a descriptive title and to use appropriate labels. Use `bins=20`, `alpha=0.5`, and `figsize=(10,5)`.\n",
    "2. The function should print a statement with the 95% confidence interval for the mean value of the given variable for each of the specified judges. See the example below for the type of statement to print, but the exact formatting is up to you.\n",
    "3. The function should return nothing.\n",
    "\n",
    "*Hints:* \n",
    "- This is designed to be a challenging question, but remember that you can use any of the functions you've already created. \n",
    "- Our solution uses an `if`-statement to assign columns named `'Mary_mean_estimate'`, `'Paul_mean_estimate'`, or `'Prue_mean_estimate'`.\n",
    "\n",
    "Here is an example output that shows a comparison of estimates for the mean number of dirty dishes produced by recipes in each of the three judges' collections.\n",
    "<img src=\"images/desired_output.jpg\" width = 700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c896d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_estimation(sample_df, judges, variable, repetitions):\n",
    "    '''Generates a histogram and for each judge, a confidence interval for the mean value of the variable from sample_df.'''\n",
    "\n",
    "# Try to replicate the graph shown in the example.\n",
    "bootstrap_estimation(technical_challenge_recipes, ['Mary', 'Paul', 'Prue'], 'Dishes', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c927c",
   "metadata": {},
   "source": [
    "**Question 5.12.** Using the `bootstrap_estimation` function you just wrote, create histograms and confidence intervals that would help you answer each of the following questions. Use `repetitions=1000`. \n",
    "\n",
    "1. Whose recipes have more sentences on average, Mary's or Paul's?\n",
    "2. Of the three judges, how do their average counts of recipe ingredients compare?\n",
    "\n",
    "For each part, all you need to do is make one call to `bootstrap_estimation` with the appropriate inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68a7993",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5_12\n",
    "manual: True\n",
    "points: 2\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d27a33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For question 1, make your function call here.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4725c76b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c0b6a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For question 2, make your function call here.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f347f",
   "metadata": {},
   "source": [
    "Feel free to explore other questions with other variables as you wish. But at this point, the devilish judge should be clear!  üòà"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b5b8c",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## Section 6: Piece of Cake? üç∞\n",
    "\n",
    "In this section of the project, we'll focus on probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff09827",
   "metadata": {},
   "source": [
    "**Question 6.1.** You wonder if it takes a lot of skill to win the bake off. If we randomly select a (series) winner from the first ten seasons of the show, what is the probability that they came first in one of the Technical Challenges? Use the `bakers` DataFrame to calculate this probability and assign your answer to the variable `p_tech_given_win`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45394e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tech_given_win = ...\n",
    "p_tech_given_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37840f7b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe9c8ec",
   "metadata": {},
   "source": [
    "**Question 6.2.** You wonder how frequently winners are recognized with the special designation of Star Baker ‚≠ê. If we randomly select a winner from the first ten seasons of the show, what is the probability that they won Star Baker ‚≠ê at some point? Assign your answer to the variable `p_star_given_win`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c749d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_star_given_win = ...\n",
    "p_star_given_win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9760d68",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a4970",
   "metadata": {},
   "source": [
    "Notice that in both of the previous questions, you calculated a conditional probability. Among bakers who satisfy one condition (winning), what is the probability they satisfy another condition (placing first in a technical, or earning Star Baker ‚≠ê). Let's generalize the code for these calculations so that we can more easily compute conditional probabilities with other conditions.\n",
    "\n",
    "**Question 6.3.** Your job is to implement the function `conditional_probability`. It has two arguments, `find` and `given`, both of which are lists. Let's walk through how it works, using an example ‚Äì suppose we want to use it to compute the probability that a randomly selected contestant from `bakers` was a Star Baker ‚≠ê, given that they won (the same probability that you computed in the previous question.)\n",
    "\n",
    "- `find` is a list of two elements:\n",
    "    - The first element in `find` is the column in `bakers` that contains the event that we are trying to find the probability of. This can be any column in `baker`; in our example, this is `'star_baker'`. \n",
    "    - The second element in `find` is the value in the aforementioned column that we're trying to find; in our example, this is `1`.\n",
    "- `given` is a list of two elements:\n",
    "    - The first element in `given` is the column in `bakers` that contains the event that we are given to be true. This can also be any column in `baker`; in our example, this is `'series_winner'`. \n",
    "    - The second element in `given` is the value in the aforementioned column; in our example, this is `1`.\n",
    "\n",
    "Putting this all together, this means that `conditional_probability(['star_baker', 1], ['series_winner', 1])` should evaluate to your answer from the previous part (but the `conditional_probability` function should work for any example, not just this one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_probability(find, given):\n",
    "    '''Returns the conditional probability of an event given a known condition.'''\n",
    "    ...\n",
    "    \n",
    "# This should evalaute to your answer to Question 6.2\n",
    "conditional_probability(['star_baker', 1], ['series_winner', 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f1b24b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f9788",
   "metadata": {},
   "source": [
    "**Question 6.4.** Now use the function `conditional_probability` to calculate the following probabilities:\n",
    "- `p_female_given_young`: The probability that a randomly chosen young contestant is female. üëßüèΩ\n",
    "- `p_female_given_elderly`: The probability that a randomly chosen elderly contestant is female. üëµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a1d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_female_given_young = ...\n",
    "p_female_given_elderly = ...\n",
    "\n",
    "# Don't change the code below.\n",
    "print(f'''P(female given young) = {p_female_given_young}\n",
    "P(female given elderly) = {p_female_given_elderly}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c7420",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fbf8c6",
   "metadata": {},
   "source": [
    "**Question 6.5.** Suppose the producers of the show want to do a special episode bringing back past contestants, as they often do for the holidays üéÑüïé. They decide to choose one contestant at random from each of the first ten seasons. What is the probability that there is at least one winner selected? Assign your answer to `p_include_winner`.\n",
    "\n",
    "*Hint:* The function `np.prod` might be helpful. Here is a [link to its documentation](https://numpy.org/doc/stable/reference/generated/numpy.prod.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfc710",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_include_winner = ...\n",
    "p_include_winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c852ea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e925b4b1",
   "metadata": {},
   "source": [
    "**Question 6.6.** You have dreams üí≠ of being on the bake off yourself, and to practice, you decide to bake 10 Technical Challenge recipes, chosen at random **with replacement** from the `technical_challenge_recipes` DataFrame. What is the probability that all 10 of them have a `'DifficultyScore'` greater than 5? Assign your answer to `p_all_above_5`.\n",
    "\n",
    "*Note:* Like all other questions in this section, this is a probability question. It does not require a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd9e89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p_all_above_5 = ...\n",
    "p_all_above_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1568db",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b30af",
   "metadata": {},
   "source": [
    "**Question 6.7.** After putting in a lot of time practicing the Technical Challenge recipes, you feel that you need to get some advice from a former participant. You originally had all their names and phone numbers ‚òéÔ∏è written down in your notebook üìì, but your dog üê∂ ate the portion of the notebook with their names, leaving you with a list of just phone numbers. You are left with no choice but to call one of them at random.\n",
    "\n",
    "But you also want quality advice, which in your mind are participants who have:\n",
    "- remained in the show for at least half a season, and\n",
    "- placed in the top 3 in at least one of the Technical Challenges.\n",
    "\n",
    "What is the probability that you will get quality advice from calling a random number from the list in your notebook? Assign your answer to `p_quality_advice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5ef04",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_quality_advice = ...\n",
    "p_quality_advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a36f72",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6_7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfffcae",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## Section 7: Recipe Name Generator üë©‚Äçüç≥üñ®Ô∏è\n",
    "\n",
    "After seeing the creative bakes featured in the Signature and Showstopper Challenges, you're feeling inspired to invent some new recipes yourself. Instead of letting your tastebuds and your better judgment guide you, you decide to generate recipe titles *randomly* in a systematic way. \n",
    "\n",
    "All of your recipe titles will consist words chosen randomly from a limited set of options. There are **three categories of words**:\n",
    "1. *Ingredients* \n",
    "    - For example, \"Chocolate\", \"Pumpkin\", and \"Mint\".\n",
    "    \n",
    "2. *Items* \n",
    "    - For example, \"Cupcakes\", \"Croissants\", and \"Biscuits\".\n",
    "    \n",
    "3. *Extras* \n",
    "    - For example, \"Meringue\", \"Swirl\", and \"Ganache\".\n",
    "\n",
    "To generate a recipe title, you'll first randomly select a template for your recipe title. There are **four recipe templates**:\n",
    "1. *Ingredient Ingredient Item with Ingredient Extra* \n",
    "    - For example, \"Chocolate Mint Cupcakes with Pumpkin Swirl\".\n",
    "2. *Item with Ingredient Extra*  \n",
    "    - For example \"Croissants with Mint Ganache\".\n",
    "3. *Ingredient, Ingredient, and Ingredient Item* \n",
    "    - For example, \"Mint, Chocolate, and Pumpkin Biscuits\".\n",
    "4. *Ingredient Ingredient Item* \n",
    "    - For example, \"Pumpkin Chocolate Croissants\".\n",
    "\n",
    "Once you have determined the template, you will randomly select *Ingredients*, *Items*, and *Extras* in the appropriate quantities. Each category of words has an associated probability distribution that describes the likelihood of each word in the category being chosen. \n",
    "\n",
    "Run the next three cells to see the possible words in each category, as well as the probability of choosing each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e5a50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ingredient_df = bpd.read_csv('data/ingredients.csv')\n",
    "ingredient_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = bpd.read_csv('data/items.csv')\n",
    "item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df = bpd.read_csv('data/extras.csv')\n",
    "extra_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748251c",
   "metadata": {},
   "source": [
    "**Question 7.1.** Write a function called `one_recipe` that generates a random recipe title using the process described above. Start by choosing one of the four possible templates at random, such that each has an equal probability of being selected. Once you have your template, select words from `ingredient_df`, `item_df`, and `extra_df` as required. \n",
    "\n",
    "If you need to select multiple ingredients, make sure to choose them **without replacement** because each ingredient should only occur once in a recipe title. For example, you should not generate \"Pumpkin Pumpkin Cupcakes\". \n",
    "\n",
    "Your function `one_recipe` should return the title of one randomly generated recipe.\n",
    "\n",
    "*Hint*: Use `np.random.choice` and take advantage of the option to specify the probability of each item being selected. See the [documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a888de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates:\n",
    "# 1. Ingredient Ingredient Item with Ingredient Extra \n",
    "# 2. Item with Ingredient Extra  \n",
    "# 3. Ingredient, Ingredient, and Ingredient Item \n",
    "# 4. Ingredient Ingredient Item\n",
    "\n",
    "def one_recipe():\n",
    "    ...\n",
    "\n",
    "one_recipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b5eca",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77693bb",
   "metadata": {},
   "source": [
    "**Question 7.2.** Generate 10,000 recipe titles and store them in an array called `recipe_titles`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145a40b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recipe_titles = ...\n",
    "recipe_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ab54b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6842799",
   "metadata": {},
   "source": [
    "**Question 7.3.** You firmly believe that chocolate makes everything better. üç´ ü©π Use the 10,000 recipe titles that you generated to estimate the probability that a randomly generated recipe title includes the word \"Chocolate\". Store your estimate in the variable `prob_chocolate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35afe6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_chocolate = ...\n",
    "prob_chocolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1617ea7d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae94d72",
   "metadata": {},
   "source": [
    "**Question 7.4.** You're also a big fan of cupcakes. üßÅ Use the 10,000 recipe titles that you generated to estimate the probability that a randomly generated recipe title includes the word \"Cupcakes\". Store your estimate in the variable `prob_cupcakes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3285af58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_cupcakes = ...\n",
    "prob_cupcakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdb3761",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55953f4",
   "metadata": {},
   "source": [
    "You should have found that your estimate for the probability of a randomly generated recipe containing the word \"Chocolate\" is significantly higher than the probability associated with the word \"Chocolate\" in `ingredient_df`. Yet, you also should have found that your estimate the probability of a randomly generated recipe containing the word \"Cupcakes\" is about the same as the probability associated with the word \"Cupcakes\" in `item_df`. \n",
    "\n",
    "Compare these values by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c208d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"The probability associated with Chocolate in the DataFrame is \"+\n",
    "      str(ingredient_df.get('probabilities').iloc[0])+\n",
    "      \" and your estimated probability of Chocolate is \"+\n",
    "      str(prob_chocolate)+\".\\n\")\n",
    "\n",
    "print(\"The probability associated with Cupcakes in the DataFrame is \"+\n",
    "      str(item_df.get('probabilities').iloc[0])+\n",
    "      \" and your estimated probability of Cupcakes is \"+\n",
    "      str(prob_cupcakes)+\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f37a45",
   "metadata": {},
   "source": [
    "**Question 7.5** Why is the probability for \"Cupcakes\" so similar to the value in the DataFrame but the probability for \"Chocolate\" so different? How can you explain this phenomenon?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92062322",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7_5\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947197a8",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de96554",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "<a id='section8'></a>\n",
    "## Section 8: Dishwashing üßºüçΩÔ∏è\n",
    "\n",
    "In this section, we will explore whether the difficulty of a recipe is correlated with the number of dirty dishes it produces. Regression is helpful when we want to use one numerical value to predict another numerical value.\n",
    "\n",
    "Let's start by visualizing the data with a scatter plot to see if linear regression would make sense for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d4076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "technical_challenge_recipes.plot(kind='scatter', x='DifficultyScore', y='Dishes');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcbd97d",
   "metadata": {},
   "source": [
    "Based on the scatter plot, it seems like linear regression would be an appropriate tool. Let's proceed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe6c03d",
   "metadata": {},
   "source": [
    "**Question 8.1.** Complete the function `standard_units` which takes in an array or Series and returns an array with the values in standard units. Then use your function to standardize the `'DifficultyScore'` and `'Dishes'` columns from `technical_challenge_recipes`. Store the standardized arrays in the variables `difficulty_standard` and `dishes_standard`. \n",
    "\n",
    "*Note*: Since the inputs to the `standard_units` function might be arrays or Series with some missing values, use [`np.nanmean`](https://numpy.org/doc/stable/reference/generated/numpy.nanmean.html) and [`np.nanstd`](https://numpy.org/doc/stable/reference/generated/numpy.nanstd.html) to compute means and standard deviations. These will ignore the missing values in the computation of the mean and standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198611ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_units(sequence):\n",
    "    '''Returns the input sequence as an array in standard units.'''\n",
    "    # Convert the input to an array, if it is not already.\n",
    "    sequence = np.array(sequence)\n",
    "    ...\n",
    "\n",
    "difficulty_standard = ...\n",
    "dishes_standard = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a25bc4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a147a6e8",
   "metadata": {},
   "source": [
    "**Question 8.2.** Complete the function `correlation`, which should take in:\n",
    "1. `df`, a DataFrame,\n",
    "2. `independent`, the column label of the independent ($x$) variable, as a string, and \n",
    "3. `dependent`, the column label of the dependent ($y$) variable, as a string.\n",
    "\n",
    "The function should output the correlation between the two variables. As before, your function needs to work even if there are missing values in the DataFrame.\n",
    "\n",
    "Then, use your function to compute the correlation between `'DifficultyScore'` and `'Dishes'` and store your result in the variable `corr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e598337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df, independent, dependent):\n",
    "    '''Returns the correlation between the independent and dependent variables in the given DataFrame.'''\n",
    "    ...\n",
    "\n",
    "corr = ...\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fee66",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece7c5b",
   "metadata": {},
   "source": [
    "**Question 8.3.** Now construct two functions, `reg_slope` and `reg_intercept`, which each take in the same three inputs as `correlation`. `reg_slope` should return the slope of the regression line and `reg_intercept` should return the intercept of the regression line, in original units. As before, your function needs to work even if there are missing values in the DataFrame.\n",
    "\n",
    "Use your function to store the slope and intercept of the regression line for  `'DifficultyScore'` and `'Dishes'` in the variables `slope` and `intercept`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d699e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_slope(df, independent, dependent):\n",
    "    '''Returns the slope of the regression line in original units.'''\n",
    "    ...\n",
    "\n",
    "def reg_intercept(df, independent, dependent):\n",
    "    '''Return the intercept of the regression line in original units.'''\n",
    "    ...\n",
    "\n",
    "slope = ...\n",
    "intercept = ...\n",
    "slope, intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef49e73",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b9292",
   "metadata": {},
   "source": [
    "**Question 8.4.** Create a function called `predict` that takes in the same three inputs as `correlation`. `predict` should return an array of predicted values of the dependent variable calculated from the regression line. \n",
    "\n",
    "Use your function to create an array of the predicted number of dirty dishes for each recipe in the `technical_challenge_recipes` DataFrame, based on the recipe's difficulty. Save your answer as `predicted_dishes`. Note that the predicted number of dirty dishes need not be a whole number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4a6b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(df, independent, dependent):\n",
    "    '''Returns an array of predicted values of the dependent variable calculated from the regression line.'''\n",
    "...\n",
    "\n",
    "predicted_dishes = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fa135",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b83035",
   "metadata": {},
   "source": [
    "**Question 8.5.** Use the strategy for overlaying scatter plots described in [this tutorial](https://www.statology.org/pandas-scatter-plot-multiple-columns/) to create an overlaid scatter plot with:\n",
    "- a blue dot üîµ for each recipe showing the difficulty on the $x$-axis and the number of dirty dishes on the $y$-axis (as in the scatter plot at the beginning of this section), and\n",
    "- a red dot üî¥ for each recipe showing the difficulty on the $x$-axis and the **predicted** number of dirty dishes on the $y$-axis.\n",
    "\n",
    "The red dots should form a straight line - that's the regression line!\n",
    "\n",
    "*Note:* This is the first time you've been asked to make an overlaid scatter plot, so you'll need to learn something new to answer this question. Read the linked tutorial carefully and try to mimic their example; you can learn how to do a lot of things this way!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407d62e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8_5\n",
    "points: 1\n",
    "manual: True\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your overlaid scatter plot here.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3760c6",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "**Question 8.6.** Use the equation of the regression line to answer the following questions. Check that your answers are reasonable using the scatter plot above. Note that the predicted number of dirty dishes need not be a whole number.\n",
    "\n",
    "1.  A recipe for cr√®me caramel üçÆ has a difficulty score of 7.5. What is the predicted number of dirty dishes for this recipe? Save your answer as `creme_caramel`.\n",
    "2. A basic recipe for chocolate chip cookies üç™ has a difficulty score of $d$ and an advanced recipe for gourmet chocolate chip cookies üç™ has a difficulty score of $d+2$. How many additional dirty dishes would we predict the advanced recipe to create, as compared to the basic one? Save your answer as `cookies`.\n",
    "3. A recipe for pretzels ü•® is predicted to create 6 dirty dishes. What is the difficulty of this recipe? Round to the nearest whole number and save your answer as `pretzels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0914682",
   "metadata": {},
   "outputs": [],
   "source": [
    "creme_caramel = ...\n",
    "cookies = ...\n",
    "pretzels = ...\n",
    "print(\"creme caramel: \"+str(creme_caramel))\n",
    "print(\"cookies: \"+str(cookies))\n",
    "print(\"pretzels: \"+str(pretzels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a630a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b9bfb",
   "metadata": {},
   "source": [
    "**Question 8.7.** Now that we have general code to calculate the regression line between any pair of variables in any DataFrame, let's generalize our code for the overlaid scatter plot so we can visualize relationships between other pairs of variables.\n",
    "\n",
    "Complete the function `display_predictions` below. This function should take in the same three inputs as the `correlation` function, create an overlaid scatter plot similar to the one in Question 8.5, and return a string describing the correlation between the variables and the slope and intercept of the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dd124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_predictions(df, independent, dependent):\n",
    "    '''Generates an overlaid scatter plot showing the relationship between the independent and dependent variables in df.\n",
    "    Returns a string describing the correlation and the slope and intercept of the regression line.'''\n",
    "    # Create your overlaid scatter plot here.\n",
    "    ...\n",
    "    # We've provided the code for the return statement.\n",
    "    return (\"The correlation between {0} and {1} is {2}. \" +\\\n",
    "           \" The slope of the regression line is {3}.\" + \\\n",
    "           \" The intercept of the regression line is {4}.\")\\\n",
    "                .format(independent, \n",
    "                        dependent, \n",
    "                        str(round(correlation(df, independent, dependent), 2)),\n",
    "                        str(round(reg_slope(df, independent, dependent), 2)), \n",
    "                        str(round(reg_intercept(df, independent, dependent), 2)))\n",
    "\n",
    "# Your function should produce the same scatter plot as in Question 8.5 on the inputs below. \n",
    "# Make sure to test it out on other inputs too.\n",
    "display_predictions(technical_challenge_recipes, 'DifficultyScore', 'Dishes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451dba65",
   "metadata": {},
   "source": [
    "**Question 8.8.** Using the `display_predictions` function you just wrote, create scatter plots and calculate regression lines that would help you answer each of the following questions. \n",
    "\n",
    "1. Do longer recipes with more sentences require more ingredients?\n",
    "    - Store the output of your call to `display_predictions` in the variable `sentences_ingredients`.\n",
    "2. Are recipes with more ingredients more difficult? \n",
    "    - Store the output of your call to `display_predictions` in the variable `ingredients_diff`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_ingredients = ...\n",
    "sentences_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce87ba4",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8_8_a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf049a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ingredients_diff = ...\n",
    "ingredients_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7151c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8_8_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a976a",
   "metadata": {},
   "source": [
    "## Finish Line üèÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa03162",
   "metadata": {},
   "source": [
    "Big congratulations! üéâ You've completed the Final Project, your last assignment for DSC 10 this quarter! Feel free to celebrate by whipping up some baked goods, like these cute [baby panda madeleines](https://www.instagram.com/p/CBDzDwsHSBW/?utm_source=ig_embed&utm_campaign=embed_video) created by Kim-Joy, a contestant from Season 9 of the Great British Bake Off.\n",
    "\n",
    "<img src=\"images/baby_pandas.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    "To submit your assignment:\n",
    "\n",
    "1. Select `Kernel -> Restart & Run All` to ensure that you have executed all cells, including the test cells. <p style=\"color: red\"><b>‚ö†Ô∏è Important!</b> We will allot 20 minutes of computer time to run your notebook. If your notebook takes longer than this to run, it may not pass the autograder! Select \"Kernel -> Restart and Run All\" to time how long your notebook takes. A notebook with correct answers should take less than 10 minutes.</p>\n",
    "\n",
    "2. Read through the notebook to make sure everything is fine and all tests passed.\n",
    "3. Run the cell below to run all tests, and make sure that they all pass.\n",
    "4. Download your notebook using File -> Download as -> Notebook (.ipynb), then upload your notebook to Gradescope. Don't forget to add your partner to your group on Gradescope!\n",
    "5. Stick around while the Gradescope autograder grades your work. Make sure you see that all tests have passed on Gradescope.\n",
    "6. Check that you have a confirmation email from Gradescope and save it as proof of your submission. \n",
    "\n",
    "If running all the tests at once causes a test to fail that didn't fail when you ran the notebook in order, check to see if you changed a variable's value later in your code. Make sure to use new variable names instead of reusing ones that are used in the tests. \n",
    "\n",
    "Remember, the tests here and on Gradescope just check the format of your answers. We will run correctness tests after the assignment's due date has passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca7781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f365b4c",
   "metadata": {},
   "source": [
    "<a id='sources'></a>\n",
    "## Data Sources üìñ\n",
    "\n",
    "- Hill A, Ismay C, Iannone R (2022). bakeoff: Data from \"The Great British Bake Off\". https://bakeoff.netlify.app/, https://github.com/apreshill/bakeoff.\n",
    "\n",
    "- Davis, Erin (2019). Are Great British Bake Off Technical Challenges getting harder? https://erdavis.com/2019/06/08/are-great-british-bake-off-technical-challenges-getting-harder/, https://gist.github.com/erdavis1/09fd4a3aa424c5425a88d47f572ec20a.\n",
    "\n",
    "- Ahamed, Nick (2019). Analyzing the Great British Bake Off. https://medium.com/analytics-vidhya/analyzing-the-great-british-bake-off-part-1-ffcdf3791bf3, https://medium.com/@nickahamed/analyzing-the-great-british-bake-off-part-2-1695ff95a0c9, https://docs.google.com/spreadsheets/d/1cvouOik_01QqtFQSq78xODIjcZZ8A-02VXa6IBvdG3I/edit#gid=0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
